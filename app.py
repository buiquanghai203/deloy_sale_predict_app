import streamlit as st
import pandas as pd
import joblib
import numpy as np
import plotly.graph_objs as go
import catboost
import gdown
from sklearn.metrics import mean_squared_log_error, r2_score
import plotly.express as px

url = 'https://drive.google.com/uc?id=1uDpF9_kJCD60aqZzXFoukxurbNhjLdAS'
output = 'models_per_family.pkl'
gdown.download(url, output, quiet=True)
def create_lags(df):
    new_df = df.copy()
    keys = ['store_nbr', 'family']
    val = 'sales'
    lags = [16, 21, 30, 45, 60, 90]

    for lag in lags:
        new_df['lag_' + str(lag)] = new_df.groupby(keys)[val].transform(lambda x: x.shift(lag))

    return new_df

def create_rolling_mean(df):
    new_df = df.sort_values(["store_nbr", "family", "date"]).copy()
    
    for i in [20]:  
        for lag in [16, 30, 60]:  # L·∫∑p qua c√°c gi√° tr·ªã lag
            new_df[f"SMA{i}_sales_lag{lag}"] = (
                new_df.groupby(["store_nbr", "family"])["sales"]
                .transform(lambda x: x.rolling(i).mean().shift(lag))
            )

    return new_df

def one_hot_encode(df, columns, nan_dummie=True, dropfirst=False): 
    original_columns = list(df.columns) 
    dummie_columns = columns 
    df = pd.get_dummies(df, columns=dummie_columns, dummy_na=nan_dummie, drop_first=dropfirst) 
    df.columns = df.columns.str.replace(" ", "_") 
    new_columns = [c for c in df.columns if c not in original_columns] 
    return df, new_columns

def main():
    st.title("H·ªá th·ªëng d·ª± b√°o doanh thu")
    st.markdown("""
    ### üìÑ H∆∞·ªõng d·∫´n ƒë·ªãnh d·∫°ng t·∫≠p tin ƒë·∫ßu v√†o (.csv)
    
    T·∫≠p tin d·ªØ li·ªáu ƒë·∫ßu v√†o c·∫ßn c√≥ c·∫•u tr√∫c gi·ªëng v·ªõi t·∫≠p hu·∫•n luy·ªán, bao g·ªìm c√°c c·ªôt sau:
    
    - `id`: M√£ ƒë·ªãnh danh duy nh·∫•t cho m·ªói d√≤ng d·ªØ li·ªáu
    - `date`: Ng√†y giao d·ªãch (ƒë·ªãnh d·∫°ng: YYYY-MM-DD)
    - `store_nbr`: S·ªë th·ª© t·ª± c·ª≠a h√†ng
    - `family`: Nh√≥m s·∫£n ph·∫©m
    - `sales`: Doanh thu (c√≥ th·ªÉ ƒë·ªÉ tr·ªëng ho·∫∑c kh√¥ng)
    - `onpromotion`: S·ªë l∆∞·ª£ng s·∫£n ph·∫©m ƒëang khuy·∫øn m√£i
    
    üìå **L∆∞u √Ω**:
    - T·∫≠p tin ph·∫£i ch·ª©a √≠t nh·∫•t **15 ng√†y d·ªØ li·ªáu g·∫ßn nh·∫•t** cho m·ªói c·ª≠a h√†ng v√† nh√≥m s·∫£n ph·∫©m.
    - N·∫øu c·ªôt `sales` cho 15 ng√†y cu·ªëi **ƒë·ªÉ tr·ªëng (NaN)** ‚Üí h·ªá th·ªëng s·∫Ω d·ª± b√°o doanh thu.
    - N·∫øu ƒë√£ c√≥ gi√° tr·ªã th·ª±c t·∫ø ‚Üí h·ªá th·ªëng s·∫Ω d·ª± b√°o doanh thu v√† v·∫Ω th√™m bi·ªÉu ƒë·ªì so s√°nh v√† t√≠nh RMSLE, R¬≤.
    
    """)
    # Upload file
    uploaded_file = st.file_uploader("Ch·ªçn t·ªáp tin CSV", type=["csv"])
    
    if uploaded_file is not None:
        # ƒê·ªçc file CSV
        df = pd.read_csv(uploaded_file)
        
        # Convert date column to datetime type
        df['date'] = pd.to_datetime(df['date'])  # Ensure the date column is in datetime format
        
        # Hi·ªÉn th·ªã dataframe
        st.write("### N·ªôi dung c·ªßa file:")
        st.dataframe(df)
        
        
        # Prepare input data for revenue prediction
        # Load additional CSV files
        df_store = pd.read_csv("stores.csv")  # Updated to use stores.csv
        local = pd.read_csv("local.csv")          # Assuming the file is named local.csv
        regional = pd.read_csv("regional.csv")    # Assuming the file is named regional.csv
        national = pd.read_csv("national.csv")    # Assuming the file is named national.csv
        events = pd.read_csv("events.csv")        # Assuming the file is named events.csv

      
        
        # Convert date columns to datetime type
        local['date'] = pd.to_datetime(local['date'])
        regional['date'] = pd.to_datetime(regional['date'])
        national['date'] = pd.to_datetime(national['date'])
        events['date'] = pd.to_datetime(events['date'])

        # Set sales for the last 15 days to NaN
        last_date = df['date'].max()
        start_date = df["date"].min() 
        last_15_days = pd.date_range(end=last_date, periods=15)
        # df.loc[df['date'].isin(last_15_days), 'sales'] = None  # Set to NaN

        

        # Merge input data with additional datasets
        sales_merged = df.merge(
            df_store, on="store_nbr", how="left",
        ).merge(
            local, on=["date", "city"], how="outer",
        ).merge(
            regional, on=["date", "state"], how="outer",
        ).merge(
            national, on="date", how="outer",
        ).merge(
            events, on="date", how="outer",
        )
        
        
     
         # Create dummy variables
        column_4 = ['holiday_local', 'holiday_regional', 'holiday_national', 'events']
        sales_merged, new_columns = one_hot_encode(df=sales_merged, columns=column_4, nan_dummie=False, dropfirst=False)
        sales_merged = sales_merged[(sales_merged['date'] <= last_date) & (sales_merged['date'] >= start_date)]
        sales_merged = sales_merged.sort_values(by=["id"])
       
        
        # Create trend variable
        sales_merged['trend'] = (sales_merged['date'] - pd.Timestamp('2013-01-01')).dt.days + 1
        
        # Create day_name feature
        sales_merged['day_name'] = sales_merged['date'].dt.strftime('%A')
        
        # Create lag features
        sales_merged = create_lags(sales_merged)
        
        # Create rolling mean features
        sales_merged = create_rolling_mean(sales_merged)
        
        
        
        # Create dummy variables
        # column_4 = ['holiday_local', 'holiday_regional', 'holiday_national', 'events']
        # sales_merged, new_columns = one_hot_encode(df=sales_merged, columns=column_4, nan_dummie=False, dropfirst=False)
        
        cols = sales_merged.columns.tolist()
        cols_reordered = cols[:10] + cols[30:] + cols[10:30]
        # S·∫Øp x·∫øp l·∫°i DataFrame
        sales_merged = sales_merged[cols_reordered]
        sales_merged = sales_merged.sort_values(by=["id"])
        # st.write(sales_merged) 
        
        # Move specified columns to category type
        cat_features  = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster', 'day_name']
        sales_merged['store_nbr'] = sales_merged['store_nbr'].astype(int)
        sales_merged['cluster'] = sales_merged['cluster'].astype(int)
        sales_merged[cat_features] = sales_merged[cat_features].astype(str)
        
        # Select all features except 'id', 'date', and 'sales' for model input (last 15 days)
        model_input = sales_merged[sales_merged['date'].isin(last_15_days)].drop(columns=['id', 'date', 'sales'])

       
        # for col in category_columns:
        #     model_input[col] = model_input[col].astype('category')
       # Ho·∫∑c .astype(int)    
        # Create child DataFrame containing only date, store_nbr, and family for the last 15 days
        child_df = sales_merged[sales_merged['date'].isin(last_15_days)][['date', 'store_nbr', 'family']]

        
        # Load the model
        models_per_family = joblib.load('models_per_family.pkl')
        
        # Prepare predictions
        predictions = []
        family_names = sales_merged['family'].unique()  # Get unique family names
        
        for i, value in enumerate(family_names):
            input_per_family = model_input[model_input['family'] == value]
            if not input_per_family.empty:     
                predict_value = models_per_family[i].predict(input_per_family)
                # Set predicted values less than 0 to 0
                predict_value[predict_value < 0] = 0
                # Apply the exponential function to un-log the values (adding 1 to avoid log(0))
                predict_value = np.expm1(predict_value)    # Use np.exp for natural logarithm
                predictions.append(predict_value)
        
        # Concatenate predictions into child_df
        for i, value in enumerate(family_names):
            input_per_family = model_input[model_input['family'] == value]
            if not input_per_family.empty:
                child_df.loc[child_df['family'] == value, 'predicted_sales'] = predictions[i]

         # Merge gi√° tr·ªã th·ª±c t·∫ø v√†o child_df ƒë·ªÉ so s√°nh
        child_df = child_df.merge(
            sales_merged[['date', 'store_nbr', 'family', 'sales']],  # C·ªôt th·ª±c t·∫ø
            on=['date', 'store_nbr', 'family'],
            how='left'
        )
        

        # Sau khi c√≥ child_df v·ªõi predicted_sales
        
        # 1. ƒê·ªïi t√™n c·ªôt th·ª±c t·∫ø
        child_df = child_df.rename(columns={'sales': 'actual_sales'})
        
        # 2. Ki·ªÉm tra d·ªØ li·ªáu th·ª±c t·∫ø c√≥ ƒë·ªß ƒë·ªÉ ƒë√°nh gi√° kh√¥ng
        valid_eval_df = child_df.dropna(subset=['actual_sales', 'predicted_sales'])
        
        # 3. T√≠nh RMSLE v√† R¬≤ n·∫øu c√≥ d·ªØ li·ªáu th·ª±c t·∫ø
        if not valid_eval_df.empty:
            rmsle = np.sqrt(mean_squared_log_error(
                valid_eval_df['actual_sales'],
                valid_eval_df['predicted_sales']
            ))
            r2 = r2_score(
                valid_eval_df['actual_sales'],
                valid_eval_df['predicted_sales']
            )
            st.write("## ƒê√°nh gi√° hi·ªáu su·∫•t")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("RMSLE", f"{rmsle:.4f}")
            with col2:
                st.metric("R¬≤ Score", f"{r2:.4f}")
            with col3:
                st.metric("S·ªë m·∫´u h·ª£p l·ªá", len(valid_eval_df))
        else:
            st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu th·ª±c t·∫ø ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t (RMSLE, R¬≤).")
        
        # -- V·∫Ω bi·ªÉu ƒë·ªì theo store v√† family ƒë√£ ch·ªçn --
        selected_store = st.selectbox("Ch·ªçn C·ª≠a h√†ng", options=sales_merged['store_nbr'].unique())
        selected_family = st.selectbox("Ch·ªçn MƒÉt h√†ng", options=sales_merged['family'].unique())
        
        if st.button("V·∫Ω bi·ªÉu ƒë·ªì d·ª± b√°o"):
            filtered_predictions = child_df[(child_df['store_nbr'] == selected_store) & (child_df['family'] == selected_family)]
        
            # Ch·ªâ ch·ªçn c√°c c·ªôt c√≥ d·ªØ li·ªáu kh√¥ng to√†n NaN
            columns_to_plot = ['predicted_sales']
            if filtered_predictions['actual_sales'].notna().any():
                columns_to_plot.append('actual_sales')
        
            # Reshape ƒë·ªÉ v·∫Ω
            df_melted = filtered_predictions.melt(
                id_vars=['date'], 
                value_vars=columns_to_plot, 
                var_name='Type', 
                value_name='Revenue'
            )
            # üí° ƒê·ªïi t√™n ƒë·ªÉ hi·ªÉn th·ªã ti·∫øng Vi·ªát trong legend
            df_melted['Type'] = df_melted['Type'].map({
                'predicted_sales': 'D·ª± b√°o',
                'actual_sales': 'Th·ª±c t·∫ø'
            })
            # V·∫Ω bi·ªÉu ƒë·ªì
            fig = px.line(df_melted, x='date', y='Revenue', color='Type',
                          title=f'Doanh thu d·ª± b√°o cho c·ª≠a h√†ng {selected_store} v√† m·∫∑t h√†ng {selected_family} trong 15 ng√†y',
                          labels={'Revenue': 'Doanh thu', 'date': 'Th·ªùi gian', 'Type': 'Legend'},
                          template='plotly_white')
        
            min_date = df_melted['date'].min()
            max_date = df_melted['date'].max()
            date_range_padding = (max_date - min_date) * 0.1
        
            fig.update_layout(
                xaxis=dict(
                    range=[min_date - date_range_padding, max_date + date_range_padding]
                )
            )
        
            st.plotly_chart(fig)




        
        # Export button
        if st.button("Xu·∫•t t·ªáp tin d·ª± b√°o ra ƒë·ªãnh d·∫°ng CSV"):
            child_df.to_csv('predicted_revenue.csv', index=False)
            st.success("Data exported successfully!")


if __name__ == "__main__":
    main()
